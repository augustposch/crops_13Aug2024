{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "515b48b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# *ml-1L-parallel-and-premade.ipynb*\n",
    "\n",
    "# Try dask and use premade train/val datasets\n",
    "\n",
    "Version 1L: Reads in premade train and val datasets for each fold. Uses Dask to run Logistic Regression models in parallel.\n",
    "\n",
    "Allows us to: specify models bank; assemble the datasets for different folds of year-wise cross-validation; run the models for 12 combinations of region, crop, compositing scheme, and in-season date; and record results (accuracies and uncertainties) in csv files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d28217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from timeit import default_timer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27790d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models_bank(architecture):\n",
    "    models_bank = {}\n",
    "    increment = 0\n",
    "    if architecture in ('RF','ET','RFET','ETRF'):\n",
    "        for n_estimators in [200, 500]:\n",
    "            for max_features in [0.05, 0.1, 0.2, 0.4, 1.0]:\n",
    "                for min_samples_split in [2, 4]:\n",
    "                    for bootstrap in [False, True]:\n",
    "                        for class_weight in [None, 'balanced']:\n",
    "\n",
    "                            increment += 1\n",
    "\n",
    "                            three_digit = str(increment).zfill(3)\n",
    "                            models_bank[three_digit] = {\n",
    "                                'n_estimators': n_estimators,\n",
    "                                'max_features': max_features,\n",
    "                                'bootstrap': bootstrap,\n",
    "                                'min_samples_split': min_samples_split,\n",
    "                                'class_weight': class_weight,\n",
    "                                'n_jobs': -1\n",
    "                            }\n",
    "\n",
    "    if architecture in ('LR','PL'):\n",
    "        # 50 regularized models\n",
    "        for l1_ratio in [0, 0.1, 0.5, 0.9, 1]:\n",
    "            for C in [1, 10**-2, 10**-4, 10**-6, 10**-8]:\n",
    "                for class_weight in [None, 'balanced']:\n",
    "                    increment += 1\n",
    "                    three_digit = str(increment).zfill(3)\n",
    "                    models_bank[three_digit] = {\n",
    "                        'solver': 'saga',\n",
    "                        'penalty': 'elasticnet',\n",
    "                        'l1_ratio': l1_ratio,\n",
    "                        'C': C,\n",
    "                        'class_weight': class_weight,\n",
    "                        'max_iter': 100000,\n",
    "                        'random_state': 19\n",
    "                    }\n",
    "        # 2 nonregularized models\n",
    "        for class_weight in [None, 'balanced']:\n",
    "            increment += 1\n",
    "            three_digit = str(increment).zfill(3)\n",
    "            models_bank[three_digit] = {\n",
    "                'solver': 'saga',\n",
    "                'penalty': 'none',\n",
    "                'class_weight': class_weight,\n",
    "                'max_iter': 20000,\n",
    "                'random_state': 19\n",
    "            }\n",
    "            \n",
    "    if architecture in ('LS'):\n",
    "        # 30 linear svms\n",
    "        for dual_penalty_loss in [(False,'l1','squared_hinge'),\n",
    "                             (False,'l2','squared_hinge'),\n",
    "                             (True,'l2','hinge')]:\n",
    "            for C in [1, 10**-2, 10**-4, 10**-6, 10**-8]:\n",
    "                for class_weight in [None, 'balanced']:\n",
    "                    increment += 1\n",
    "                    three_digit = str(increment).zfill(3)\n",
    "                    models_bank[three_digit] = {\n",
    "                        'dual': dual_penalty_loss[0],\n",
    "                        'penalty': dual_penalty_loss[1],\n",
    "                        'loss': dual_penalty_loss[2],\n",
    "                        'C': C,\n",
    "                        'class_weight': class_weight,\n",
    "                        'max_iter': 100000,\n",
    "                    }\n",
    "                    \n",
    "    if architecture in ('SV'):\n",
    "        # 150 nonlinear svms\n",
    "        for kernel in ['rbf','poly','sigmoid']:\n",
    "            for C in [1, 10**-2, 10**-4, 10**-6, 10**-8]:\n",
    "                for gamma in [100, 10, 1, 10**-1, 10**-2]:\n",
    "                    for class_weight in [None, 'balanced']:\n",
    "                        increment += 1\n",
    "                        three_digit = str(increment).zfill(3)\n",
    "                        models_bank[three_digit] = {\n",
    "                                     'kernel': kernel,\n",
    "                                     'C': C,\n",
    "                                     'gamma': gamma,\n",
    "                                     'degree': 3,\n",
    "                                     'coef0': 0,\n",
    "                                     'class_weight': class_weight,\n",
    "                                     'max_iter': -1,\n",
    "                                                }\n",
    "                        \n",
    "    if architecture in ('NB'):\n",
    "        # 7 Naive Bayes\n",
    "        for var_smoothing in [10**-6, 10**-7, 10**-8,\n",
    "                              10**-9, 10**-10, 10**-11, 0]:\n",
    "            increment += 1\n",
    "            three_digit = str(increment).zfill(3)\n",
    "            models_bank[three_digit] = {\n",
    "                         'var_smoothing': var_smoothing\n",
    "                                    }\n",
    "            \n",
    "    if architecture in ('KN','PK'):\n",
    "        step1 = [int(np.round(1.4**x)) for x in range(7,21)]\n",
    "        step2 = [x+1 if x%2==0 else x for x in step1]\n",
    "        k_list = [1,3,5,7,9] + step2\n",
    "        # 152 models\n",
    "        for n_neighbors in k_list:\n",
    "            for weights in ['uniform','distance']:\n",
    "                for metric in ['manhattan','euclidean','chebyshev','canberra']:\n",
    "                    increment += 1\n",
    "                    three_digit = str(increment).zfill(3)\n",
    "                    models_bank[three_digit] = {\n",
    "                        'n_neighbors': n_neighbors,\n",
    "                        'weights': weights,\n",
    "                        'metric': metric\n",
    "                    }\n",
    "         \n",
    "    if architecture in ('NP'): # \"Neural: Perceptron\"\n",
    "        # 36 neural perceptrons\n",
    "        for hidden_layer_sizes in [(100,), (100,100,100)]:\n",
    "            for alpha in [1, 10**-2, 10**-4, 10**-6, 10**-8, 10**-10]:\n",
    "                for activation in ['relu', 'logistic', 'tanh']:\n",
    "                    increment += 1\n",
    "                    three_digit = str(increment).zfill(3)\n",
    "                    models_bank[three_digit] = {'solver': 'adam',\n",
    "                        'hidden_layer_sizes': hidden_layer_sizes, \n",
    "                        'activation': activation,\n",
    "                        'alpha': alpha,\n",
    "                        'learning_rate_init': 0.001,\n",
    "                        'random_state': 19,\n",
    "                        'n_iter_no_change': 10,\n",
    "                        'max_iter': 2000}\n",
    "    \n",
    "    return models_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08591a30-9eb0-4188-8946-61392af59637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_premade_X_y_train_val(training_sample_size,validation_sample_size,tile,\n",
    "                     val_year,scheme_name,crop_of_interest_id,in_season):\n",
    "\n",
    "    loc = f'../data/premade_{training_sample_size}_{validation_sample_size}'\n",
    "    strings = []\n",
    "    for arg in [tile,val_year,scheme_name,crop_of_interest_id,in_season]:\n",
    "        strings.append(f'{arg}')\n",
    "    most_of_name = '_'.join(strings) \n",
    "    \n",
    "    Xy_trainval= ['X_train', 'X_val', 'y_train', 'y_val']\n",
    "    \n",
    "    d = {}\n",
    "    for spec in Xy_trainval:\n",
    "        d[spec] = np.load(f'{loc}/{most_of_name}_{spec}.npy')\n",
    "    \n",
    "    return d['X_train'], d['X_val'], d['y_train'], d['y_val']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff9b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_predict_report(model_name,\n",
    "                      model,\n",
    "                      training_sample_size,\n",
    "                      validation_sample_size,\n",
    "                      tile,\n",
    "                      years,\n",
    "                      scheme_name,\n",
    "                      crop_of_interest_id,\n",
    "                      in_season,\n",
    "                      from_premade=True\n",
    "                      ):\n",
    "    \n",
    "    # produce csv_name\n",
    "    exempt = ['years', 'model']\n",
    "    param_value_strings = [f'{model_name}',\n",
    "                      f'{training_sample_size}',\n",
    "                      f'{validation_sample_size}',\n",
    "                      f'{tile}',\n",
    "                      f'{scheme_name}',\n",
    "                      f'{crop_of_interest_id}',\n",
    "                      f'{in_season}']\n",
    "    csv_name = '_'.join(param_value_strings) +'.csv'\n",
    "\n",
    "    # check whether previously run and, if so, end the effort\n",
    "    if csv_name in os.listdir('../data/results/'):\n",
    "        return 'If you see this, the specified model was previously run.'\n",
    "\n",
    "    print(f'-- Process for {csv_name} --')\n",
    "    \n",
    "    # below is actually fitting and predicting and reporting\n",
    "    \n",
    "    conf = []\n",
    "\n",
    "    for val_year in years:\n",
    "        print('Starting a fold...')\n",
    "        print('> Assembling the datasets')\n",
    "        # NEED: X_train, y_train, X_val, y_val\n",
    "        if from_premade==True: \n",
    "            X_train, X_val, y_train, y_val = grab_premade_X_y_train_val(training_sample_size,\n",
    "                    validation_sample_size,tile,val_year,scheme_name,\n",
    "                    crop_of_interest_id,in_season)\n",
    "        \n",
    "        if from_premade!=True:\n",
    "            return 'This function in this notebook is only for from_premade=True'\n",
    "        \n",
    "        print('> Fitting the model on the training set')\n",
    "        model.fit(X_train, y_train)\n",
    "        print('> Predicting on the validation set')\n",
    "        pred = model.predict(X_val)\n",
    "\n",
    "        print('> Recording performance metrics')\n",
    "        act = y_val\n",
    "        ActPred_00 = sum((act==0) & (pred==0))\n",
    "        ActPred_01 = sum((act==0) & (pred==1))\n",
    "        ActPred_10 = sum((act==1) & (pred==0))\n",
    "        ActPred_11 = sum((act==1) & (pred==1))\n",
    "        conf_1yr = [ActPred_00, ActPred_01, ActPred_10, ActPred_11]\n",
    "\n",
    "        conf.append(conf_1yr)\n",
    "        print('Finished a fold.')\n",
    "\n",
    "    carr = np.array(conf)\n",
    "\n",
    "    carr = np.row_stack([carr,np.full((2,4),-1)])\n",
    "\n",
    "    # above we added the totals row\n",
    "    # now we need to add the columns for precision and recall\n",
    "\n",
    "    # create dataframe\n",
    "    cdf = pd.DataFrame(data = carr,\n",
    "                      index = [f'ValYear{yr}' for yr in years]+['Mean','StdE'],\n",
    "                      columns = ['ActPred_00', 'ActPred_01', \n",
    "                                 'ActPred_10', 'ActPred_11']\n",
    "                      )\n",
    "\n",
    "    cdf['Precision'] = cdf.ActPred_11 / (cdf.ActPred_01 + cdf.ActPred_11)\n",
    "    cdf['Recall'] = cdf.ActPred_11 / (cdf.ActPred_10 + cdf.ActPred_11)\n",
    "    cdf['F1'] = 2*cdf.Precision*cdf.Recall / (cdf.Precision + cdf.Recall)\n",
    "    for col in ['Precision','Recall','F1']:\n",
    "        cdf.at['Mean',col] = np.mean(cdf.loc[:'ValYear2022',col])\n",
    "        cdf.at['StdE',col] = np.std(cdf.loc[:'ValYear2022',col])\n",
    "    \n",
    "    \n",
    "    param_strings = [f'# model_name: {model_name}',\n",
    "                     f'# model: {model}',\n",
    "                      f'# training_sample_size: {training_sample_size}',\n",
    "                      f'# validation_sample_size: {validation_sample_size}',\n",
    "                      f'# tile: {tile}',\n",
    "                      f'# scheme_name: {scheme_name}',\n",
    "                      f'# crop_of_interest_id: {crop_of_interest_id}',\n",
    "                      f'# in_season: {in_season}']\n",
    "    comment = '\\n'.join(param_strings) + '\\n' \n",
    "    with open(f'../data/results/{csv_name}', 'a') as f:\n",
    "        f.write(comment)\n",
    "        cdf.to_csv(f)\n",
    "    \n",
    "    print(f'Find results in ../data/results/{csv_name}')\n",
    "    \n",
    "    return f'Find results in ../data/results/{csv_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520e105-4ab3-4aa6-8ad3-016e71032cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2516d8df-2e0f-43fd-b41e-696c1f72aa49",
   "metadata": {},
   "source": [
    "## Dask initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec56425-bb02-49cd-b9d1-94813058e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import print\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1b719-a689-41be-b670-032c3f3883ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client(#memory_limit='128GiB',\n",
    "                #threads_per_worker=1,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f5f4e-1135-43f9-985c-2bf9fca2ed01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40429aac-e2ed-468e-a3cc-0f2f436ba05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796ee59-3c15-4c21-bf40-f63a544455c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_predict_report = dask.delayed(fit_predict_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839ae93-c06c-49c8-8a46-74af6153e549",
   "metadata": {},
   "source": [
    "### Run all the models in parallel using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df496302-f49d-4c1f-96a8-35f1c983a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = default_timer()\n",
    "\n",
    "models_bank = create_models_bank('LS')\n",
    "training_sample_size = 0.001\n",
    "validation_sample_size = 0.001\n",
    "\n",
    "r = []\n",
    "for tile_coiid in [('10SFH',75),('15TVG',1)]:\n",
    "    for scheme_name in ['14day','5day']:\n",
    "        for in_season in [160, 230, None]:\n",
    "            for three_digit in models_bank.keys():\n",
    "\n",
    "                p = {\n",
    "\n",
    "                ## SPECIFY MODEL ##\n",
    "                'model_name': 'LS' + three_digit,\n",
    "                'model': make_pipeline(StandardScaler(),\n",
    "                                       LinearSVC(**models_bank[three_digit])\n",
    "                                      ),\n",
    "                'training_sample_size': training_sample_size,\n",
    "                'validation_sample_size': validation_sample_size,\n",
    "\n",
    "                ## SPECIFY TILE AND SCHEME ##\n",
    "                'tile': tile_coiid[0],\n",
    "                'years': [2018, 2019, 2020, 2021, 2022],\n",
    "                'scheme_name': scheme_name,\n",
    "                'crop_of_interest_id': tile_coiid[1], \n",
    "                'in_season': in_season\n",
    "                }\n",
    "\n",
    "                #fit_predict_report(**p) # run with the above parameters\n",
    "                r.append(fit_predict_report(**p))\n",
    "                \n",
    "dask.compute(*r)\n",
    "\n",
    "duration = default_timer() - start\n",
    "print(duration)\n",
    "with open(f'../data/times/time{start}.txt', 'a') as f:\n",
    "    f.write(str(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4a689-8452-48b4-844a-a3d268ce4c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = default_timer()\n",
    "\n",
    "architecture = 'NP'  ## SPECIFY HERE\n",
    "\n",
    "models_bank = create_models_bank(architecture)\n",
    "training_sample_size = 0.001\n",
    "validation_sample_size = 0.001\n",
    "\n",
    "r = []\n",
    "for tile_coiid in [('10SFH',75),('15TVG',1)]:\n",
    "    for scheme_name in ['14day','5day']:\n",
    "        for in_season in [160, 230, None]:\n",
    "            for three_digit in models_bank.keys():\n",
    "\n",
    "                p = {\n",
    "\n",
    "                ## SPECIFY MODEL ##\n",
    "                'model_name': architecture + three_digit,\n",
    "                'model': make_pipeline(StandardScaler(),  ## SPECIFY HERE\n",
    "                                       #PCA(0.9),\n",
    "                                       MLPClassifier(**models_bank[three_digit])\n",
    "                                      ),\n",
    "                'training_sample_size': training_sample_size,\n",
    "                'validation_sample_size': validation_sample_size,\n",
    "\n",
    "                ## SPECIFY TILE AND SCHEME ##\n",
    "                'tile': tile_coiid[0],\n",
    "                'years': [2018, 2019, 2020, 2021, 2022],\n",
    "                'scheme_name': scheme_name,\n",
    "                'crop_of_interest_id': tile_coiid[1], \n",
    "                'in_season': in_season\n",
    "                }\n",
    "\n",
    "                #fit_predict_report(**p) # run with the above parameters\n",
    "                r.append(fit_predict_report(**p))\n",
    "                \n",
    "dask.compute(*r)\n",
    "\n",
    "duration = default_timer() - start\n",
    "print(duration)\n",
    "with open(f'../data/times/time{start}.txt', 'a') as f:\n",
    "    f.write(str(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f53560b-4a09-4cdc-88bd-93ef7fdffea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6e8fc-9f10-4500-aa69-ec4203ef7c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075f4d3-97e3-4a70-bbd4-2d2a9ee066e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d43a26-e4ed-4bf1-918f-baeedcb06d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714b2b4-f1e5-4c0f-a168-c4f8eac67c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84e948-7b44-438a-bddf-623a4dd58b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecf38ed9-208c-48b9-8489-1c35f0b1cdca",
   "metadata": {},
   "source": [
    "# Scratch trial and error: testing ground to get a sense of best hyperparameters to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6e61b-346e-4e95-a61a-8ffea83f253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = default_timer()\n",
    "\n",
    "# Here, adjust the hyperparameters in h and run by trial and error.\n",
    "h = {'n_neighbors': 9,\n",
    "     'p': 1.5,  # p is the dimension/norm of the distance metric\n",
    "    } \n",
    "\n",
    "training_sample_size = 0.001\n",
    "validation_sample_size = 0.001\n",
    "\n",
    "r = []\n",
    "for tile_coiid in [('10SFH',75)]:\n",
    "    for scheme_name in ['14day']:\n",
    "        for in_season in [160]:\n",
    "            for hyperparams in [h]:\n",
    "\n",
    "                p = {\n",
    "\n",
    "                ## SPECIFY MODEL ##\n",
    "                'model_name': 'testzKNN_9-1.5', #+ three_digit,\n",
    "                'model': make_pipeline(StandardScaler(),\n",
    "                                       KNeighborsClassifier(**hyperparams)\n",
    "                                      ),\n",
    "                'training_sample_size': training_sample_size,\n",
    "                'validation_sample_size': validation_sample_size,\n",
    "\n",
    "                ## SPECIFY TILE AND SCHEME ##\n",
    "                'tile': tile_coiid[0],\n",
    "                'years': [2018, 2019, 2020, 2021, 2022],\n",
    "                'scheme_name': scheme_name,\n",
    "                'crop_of_interest_id': tile_coiid[1], \n",
    "                'in_season': in_season\n",
    "                }\n",
    "\n",
    "                #fit_predict_report(**p) # run with the above parameters\n",
    "                r.append(fit_predict_report(**p))\n",
    "                \n",
    "dask.compute(*r)\n",
    "\n",
    "duration = default_timer() - start\n",
    "print(duration)\n",
    "with open(f'../data/time{start}.txt', 'a') as f:\n",
    "    f.write(str(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422a3fe-63f4-4397-add3-8a30ff513f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = default_timer()\n",
    "\n",
    "# Here, adjust the hyperparameters in h and run by trial and error.\n",
    "h = {'solver': 'adam',\n",
    "'hidden_layer_sizes':(100,100,100), # try a couple\n",
    "'activation':'relu', # relu, logistic, tanh\n",
    "'alpha':10**-6, #try a nice list of values - try [10**-2, 10**-4, ..., 10**-10]\n",
    "'learning_rate_init': 0.001,\n",
    "'random_state': 19,\n",
    "'n_iter_no_change': 10,\n",
    "'max_iter':1000\n",
    "} \n",
    "\n",
    "# can always do a second grid if the first grid doesn't find good stuff\n",
    "\n",
    "training_sample_size = 0.001\n",
    "validation_sample_size = 0.001\n",
    "\n",
    "r = []\n",
    "for tile_coiid in [('10SFH',75)]:\n",
    "    for scheme_name in ['14day']:\n",
    "        for in_season in [160]:\n",
    "            for hyperparams in [h]:\n",
    "\n",
    "                p = {\n",
    "\n",
    "                ## SPECIFY MODEL ##\n",
    "                'model_name': 'testzNN_D', #+ three_digit,\n",
    "                'model': make_pipeline(StandardScaler(),\n",
    "                                       MLPClassifier(**hyperparams)\n",
    "                                      ),\n",
    "                'training_sample_size': training_sample_size,\n",
    "                'validation_sample_size': validation_sample_size,\n",
    "\n",
    "                ## SPECIFY TILE AND SCHEME ##\n",
    "                'tile': tile_coiid[0],\n",
    "                'years': [2018, 2019, 2020, 2021, 2022],\n",
    "                'scheme_name': scheme_name,\n",
    "                'crop_of_interest_id': tile_coiid[1], \n",
    "                'in_season': in_season\n",
    "                }\n",
    "\n",
    "                #fit_predict_report(**p) # run with the above parameters\n",
    "                r.append(fit_predict_report(**p))\n",
    "                \n",
    "dask.compute(*r)\n",
    "\n",
    "duration = default_timer() - start\n",
    "print(duration)\n",
    "with open(f'../data/time{start}.txt', 'a') as f:\n",
    "    f.write(str(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00515e3d-30f8-43ac-af41-2073faf8f843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7a742-170b-4931-847d-4575b141f74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab6e24-5402-4a7d-9a43-2bb1ef400f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c0b9f-a56e-4bc6-b89f-da8db0d762bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b739ad-92d8-4ec5-ba2b-88420a4fec26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
