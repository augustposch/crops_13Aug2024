{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "515b48b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# *ml-4A-use-all-data.ipynb*\n",
    "\n",
    "Version 4A: Reads in full, 100% train and 0.1% val datasets for each fold.\n",
    "\n",
    "Uses Bagging classifier to handle all the 100% of data (52 million observations in each training fold)\n",
    "\n",
    "The models we're interested in running include: Logistic Regression, Linear SVM, Nonlinear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb15c4ad-0a56-4f58-8c8d-8847187dfd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from timeit import default_timer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a59ba9d-c61d-4173-8479-87c19f2d8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models_bank(architecture):\n",
    "    models_bank = {}\n",
    "    increment = 0\n",
    "    if architecture in ('RF','ET','RFET','ETRF'):\n",
    "        for n_estimators in [200, 500]:\n",
    "            for max_features in [0.05, 0.1, 0.2, 0.4, 1.0]:\n",
    "                for min_samples_split in [2, 4]:\n",
    "                    for bootstrap in [False, True]:\n",
    "                        for class_weight in [None, 'balanced']:\n",
    "\n",
    "                            increment += 1\n",
    "\n",
    "                            three_digit = str(increment).zfill(3)\n",
    "                            models_bank[three_digit] = {\n",
    "                                'n_estimators': n_estimators,\n",
    "                                'max_features': max_features,\n",
    "                                'bootstrap': bootstrap,\n",
    "                                'min_samples_split': min_samples_split,\n",
    "                                'class_weight': class_weight,\n",
    "                                'n_jobs': -1\n",
    "                            }\n",
    "\n",
    "    if architecture in ('LR','PL'):\n",
    "        # 50 regularized models\n",
    "        for l1_ratio in [0, 0.1, 0.5, 0.9, 1]:\n",
    "            for C in [1, 10**-2, 10**-4, 10**-6, 10**-8]:\n",
    "                for class_weight in [None, 'balanced']:\n",
    "                    increment += 1\n",
    "                    three_digit = str(increment).zfill(3)\n",
    "                    models_bank[three_digit] = {\n",
    "                        'solver': 'saga',\n",
    "                        'penalty': 'elasticnet',\n",
    "                        'l1_ratio': l1_ratio,\n",
    "                        'C': C,\n",
    "                        'class_weight': class_weight,\n",
    "                        'max_iter': 100000,\n",
    "                        'random_state': 19\n",
    "                    }\n",
    "        # 2 nonregularized models\n",
    "        for class_weight in [None, 'balanced']:\n",
    "            increment += 1\n",
    "            three_digit = str(increment).zfill(3)\n",
    "            models_bank[three_digit] = {\n",
    "                'solver': 'saga',\n",
    "                'penalty': 'none',\n",
    "                'class_weight': class_weight,\n",
    "                'max_iter': 20000,\n",
    "                'random_state': 19\n",
    "            }\n",
    "            \n",
    "    if architecture in ('LS'):\n",
    "        # 30 linear svms\n",
    "        for dual_penalty_loss in [(False,'l1','squared_hinge'),\n",
    "                             (False,'l2','squared_hinge'),\n",
    "                             (True,'l2','hinge')]:\n",
    "            for C in [1, 10**-2, 10**-4, 10**-6, 10**-8]:\n",
    "                for class_weight in [None, 'balanced']:\n",
    "                    increment += 1\n",
    "                    three_digit = str(increment).zfill(3)\n",
    "                    models_bank[three_digit] = {\n",
    "                        'dual': dual_penalty_loss[0],\n",
    "                        'penalty': dual_penalty_loss[1],\n",
    "                        'loss': dual_penalty_loss[2],\n",
    "                        'C': C,\n",
    "                        'class_weight': class_weight,\n",
    "                        'max_iter': 100000,\n",
    "                    }\n",
    "                    \n",
    "    if architecture in ('SV'):\n",
    "        # 150 nonlinear svms\n",
    "        for kernel in ['rbf','poly','sigmoid']:\n",
    "            for C in [1, 10**-2, 10**-4, 10**-6, 10**-8]:\n",
    "                for gamma in [100, 10, 1, 10**-1, 10**-2]:\n",
    "                    for class_weight in [None, 'balanced']:\n",
    "                        increment += 1\n",
    "                        three_digit = str(increment).zfill(3)\n",
    "                        models_bank[three_digit] = {\n",
    "                                     'kernel': kernel,\n",
    "                                     'C': C,\n",
    "                                     'gamma': gamma,\n",
    "                                     'degree': 3,\n",
    "                                     'coef0': 0,\n",
    "                                     'class_weight': class_weight,\n",
    "                                     'max_iter': -1,\n",
    "                                                }\n",
    "                        \n",
    "    if architecture in ('NB'):\n",
    "        # 7 Naive Bayes\n",
    "        for var_smoothing in [10**-6, 10**-7, 10**-8,\n",
    "                              10**-9, 10**-10, 10**-11, 0]:\n",
    "            increment += 1\n",
    "            three_digit = str(increment).zfill(3)\n",
    "            models_bank[three_digit] = {\n",
    "                         'var_smoothing': var_smoothing\n",
    "                                    }\n",
    "            \n",
    "    if architecture in ('KN','PK'):\n",
    "        step1 = [int(np.round(1.4**x)) for x in range(7,21)]\n",
    "        step2 = [x+1 if x%2==0 else x for x in step1]\n",
    "        k_list = [1,3,5,7,9] + step2\n",
    "        # 152 models\n",
    "        for n_neighbors in k_list:\n",
    "            for weights in ['uniform','distance']:\n",
    "                for metric in ['manhattan','euclidean','chebyshev','canberra']:\n",
    "                    increment += 1\n",
    "                    three_digit = str(increment).zfill(3)\n",
    "                    models_bank[three_digit] = {\n",
    "                        'n_neighbors': n_neighbors,\n",
    "                        'weights': weights,\n",
    "                        'metric': metric\n",
    "                    }\n",
    "         \n",
    "    if architecture in ('NP'): # \"Neural: Perceptron\"\n",
    "        # 36 neural perceptrons\n",
    "        for hidden_layer_sizes in [(100,), (100,100,100)]:\n",
    "            for alpha in [1, 10**-2, 10**-4, 10**-6, 10**-8, 10**-10]:\n",
    "                for activation in ['relu', 'logistic', 'tanh']:\n",
    "                    increment += 1\n",
    "                    three_digit = str(increment).zfill(3)\n",
    "                    models_bank[three_digit] = {'solver': 'adam',\n",
    "                        'hidden_layer_sizes': hidden_layer_sizes, \n",
    "                        'activation': activation,\n",
    "                        'alpha': alpha,\n",
    "                        'learning_rate_init': 0.001,\n",
    "                        'random_state': 19,\n",
    "                        'n_iter_no_change': 10,\n",
    "                        'max_iter': 2000}\n",
    "    \n",
    "    return models_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2131f54a-792b-4cc3-b074-19b397f4d60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28be53e9-75a6-4b4f-8028-d31c4245a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_premade_X_y_train_val(training_sample_size,validation_sample_size,tile,\n",
    "                     val_year,scheme_name,crop_of_interest_id,in_season):\n",
    "\n",
    "    loc = f'../data/premade_{training_sample_size}_{validation_sample_size}'\n",
    "    strings = []\n",
    "    for arg in [tile,val_year,scheme_name,crop_of_interest_id,in_season]:\n",
    "        strings.append(f'{arg}')\n",
    "    most_of_name = '_'.join(strings) \n",
    "    \n",
    "    Xy_trainval= ['X_train', 'X_val', 'y_train', 'y_val']\n",
    "    \n",
    "    d = {}\n",
    "    for spec in Xy_trainval:\n",
    "        d[spec] = np.load(f'{loc}/{most_of_name}_{spec}.npy')\n",
    "    \n",
    "    return d['X_train'], d['X_val'], d['y_train'], d['y_val']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed6f5b47-8eee-406f-b0ea-0d723014309d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_predict_report(model_name,\n",
    "                      model,\n",
    "                      training_sample_size,\n",
    "                      validation_sample_size,\n",
    "                      tile,\n",
    "                      years,\n",
    "                      scheme_name,\n",
    "                      crop_of_interest_id,\n",
    "                      in_season,\n",
    "                      from_premade=True\n",
    "                      ):\n",
    "    \n",
    "    # produce csv_name\n",
    "    exempt = ['years', 'model']\n",
    "    param_value_strings = [f'{model_name}',\n",
    "                      f'{training_sample_size}',\n",
    "                      f'{validation_sample_size}',\n",
    "                      f'{tile}',\n",
    "                      f'{scheme_name}',\n",
    "                      f'{crop_of_interest_id}',\n",
    "                      f'{in_season}']\n",
    "    csv_name = '_'.join(param_value_strings) +'.csv'\n",
    "\n",
    "    # check whether previously run and, if so, end the effort\n",
    "    if csv_name in os.listdir('../data/results/'):\n",
    "        return 'If you see this, the specified model was previously run.'\n",
    "\n",
    "    print(f'-- Process for {csv_name} --')\n",
    "    \n",
    "    # below is actually fitting and predicting and reporting\n",
    "    \n",
    "    conf = []\n",
    "\n",
    "    for val_year in years:\n",
    "        print('Starting a fold...')\n",
    "        print('> Assembling the datasets')\n",
    "        # NEED: X_train, y_train, X_val, y_val\n",
    "        if from_premade==True: \n",
    "            X_train, X_val, y_train, y_val = grab_premade_X_y_train_val(training_sample_size,\n",
    "                    validation_sample_size,tile,val_year,scheme_name,\n",
    "                    crop_of_interest_id,in_season)\n",
    "        \n",
    "        if from_premade!=True:\n",
    "            return 'This function in this notebook is only for from_premade=True'\n",
    "        \n",
    "        print('> Fitting the model on the training set')\n",
    "        model.fit(X_train, y_train)\n",
    "        print('> Predicting on the validation set')\n",
    "        pred = model.predict(X_val)\n",
    "\n",
    "        print('> Recording performance metrics')\n",
    "        act = y_val\n",
    "        ActPred_00 = sum((act==0) & (pred==0))\n",
    "        ActPred_01 = sum((act==0) & (pred==1))\n",
    "        ActPred_10 = sum((act==1) & (pred==0))\n",
    "        ActPred_11 = sum((act==1) & (pred==1))\n",
    "        conf_1yr = [ActPred_00, ActPred_01, ActPred_10, ActPred_11]\n",
    "\n",
    "        conf.append(conf_1yr)\n",
    "        print('Finished a fold.')\n",
    "\n",
    "    carr = np.array(conf)\n",
    "\n",
    "    carr = np.row_stack([carr,np.full((2,4),-1)])\n",
    "\n",
    "    # above we added the totals row\n",
    "    # now we need to add the columns for precision and recall\n",
    "\n",
    "    # create dataframe\n",
    "    cdf = pd.DataFrame(data = carr,\n",
    "                      index = [f'ValYear{yr}' for yr in years]+['Mean','StdE'],\n",
    "                      columns = ['ActPred_00', 'ActPred_01', \n",
    "                                 'ActPred_10', 'ActPred_11']\n",
    "                      )\n",
    "\n",
    "    cdf['Precision'] = cdf.ActPred_11 / (cdf.ActPred_01 + cdf.ActPred_11)\n",
    "    cdf['Recall'] = cdf.ActPred_11 / (cdf.ActPred_10 + cdf.ActPred_11)\n",
    "    cdf['F1'] = 2*cdf.Precision*cdf.Recall / (cdf.Precision + cdf.Recall)\n",
    "    for col in ['Precision','Recall','F1']:\n",
    "        cdf.at['Mean',col] = np.mean(cdf.loc[:'ValYear2022',col])\n",
    "        cdf.at['StdE',col] = np.std(cdf.loc[:'ValYear2022',col])\n",
    "    \n",
    "    \n",
    "    param_strings = [f'# model_name: {model_name}',\n",
    "                     f'# model: {model}',\n",
    "                      f'# training_sample_size: {training_sample_size}',\n",
    "                      f'# validation_sample_size: {validation_sample_size}',\n",
    "                      f'# tile: {tile}',\n",
    "                      f'# scheme_name: {scheme_name}',\n",
    "                      f'# crop_of_interest_id: {crop_of_interest_id}',\n",
    "                      f'# in_season: {in_season}']\n",
    "    comment = '\\n'.join(param_strings) + '\\n' \n",
    "    with open(f'../data/results/{csv_name}', 'a') as f:\n",
    "        f.write(comment)\n",
    "        cdf.to_csv(f)\n",
    "    \n",
    "    print(f'Find results in ../data/results/{csv_name}')\n",
    "    \n",
    "    return f'Find results in ../data/results/{csv_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855d5063-c14b-4997-882d-3d4cfbca53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_model_object(model_name,\n",
    "                        bagged,\n",
    "                        n_estimators=1000,\n",
    "                        sample_size=0.001,\n",
    "                        bootstrap=True):\n",
    "    architecture = model_name[:2]\n",
    "    three_digit = model_name[2:]\n",
    "    models_bank = create_models_bank(architecture)\n",
    "    hp = models_bank[three_digit] # hyperparameters\n",
    "        \n",
    "    if architecture in ('RF'):\n",
    "        model = RandomForestClassifier(**hp)\n",
    "        \n",
    "    if architecture in ('ET'):\n",
    "        model = ExtraTreesClassifier(**hp)\n",
    "        \n",
    "    if architecture in ('LR'):\n",
    "        model = make_pipeline(StandardScaler(),\n",
    "                              LogisticRegression(**hp)\n",
    "                              )\n",
    "        \n",
    "    if architecture in ('PL'):\n",
    "        model = make_pipeline(StandardScaler(),\n",
    "                              PCA(0.9),\n",
    "                              LogisticRegression(**hp)\n",
    "                              )\n",
    "       \n",
    "    if architecture in ('LS'):\n",
    "        model = make_pipeline(StandardScaler(),\n",
    "                              LinearSVC(**hp)\n",
    "                             )\n",
    "        \n",
    "    if architecture in ('SV'):\n",
    "        model = make_pipeline(StandardScaler(),\n",
    "                              SVC(**hp)\n",
    "                             )\n",
    "       \n",
    "    if architecture in ('NB'):\n",
    "        model = GaussianNB(**hp)\n",
    "       \n",
    "    if architecture in ('KN'):\n",
    "        model = make_pipeline(StandardScaler(),\n",
    "                              KNeighborsClassifier(**hp)\n",
    "                             )\n",
    "        \n",
    "    if architecture in ('PK'):\n",
    "        model = make_pipeline(StandardScaler(),\n",
    "                              PCA(0.9),\n",
    "                              KNeighborsClassifier(**hp)\n",
    "                             )\n",
    "       \n",
    "    if architecture in ('NP'):\n",
    "        model = make_pipeline(StandardScaler(),\n",
    "                       MLPClassifier(**hp)\n",
    "                      )\n",
    "       \n",
    "    if not bagged:\n",
    "        return model\n",
    "    \n",
    "    if bagged:\n",
    "        return BaggingClassifier(base_estimator=model, # choose which model\n",
    "                                n_estimators=n_estimators, # train many estimators\n",
    "                                max_samples=sample_size, # train each estimator on small sample\n",
    "                                max_features=1.0, # use all the features\n",
    "                                bootstrap=bootstrap, # draw samples with replacement\n",
    "                                bootstrap_features=False,\n",
    "                                n_jobs=-1,\n",
    "                                random_state=19)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839ae93-c06c-49c8-8a46-74af6153e549",
   "metadata": {},
   "source": [
    "### Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9580f59-cf3e-4119-865c-5d26c9495079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Process for SV130_1.0_0.001_10SFH_14day_75_160.csv --\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Find results in ../data/results/SV130_1.0_0.001_10SFH_14day_75_160.csv\n",
      "-- Process for SV085_1.0_0.001_10SFH_14day_75_160.csv --\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Find results in ../data/results/SV085_1.0_0.001_10SFH_14day_75_160.csv\n",
      "-- Process for SV059_1.0_0.001_10SFH_14day_75_160.csv --\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Find results in ../data/results/SV059_1.0_0.001_10SFH_14day_75_160.csv\n",
      "-- Process for LS001_1.0_0.001_10SFH_14day_75_160.csv --\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Find results in ../data/results/LS001_1.0_0.001_10SFH_14day_75_160.csv\n",
      "-- Process for LR043_1.0_0.001_10SFH_14day_75_160.csv --\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Find results in ../data/results/LR043_1.0_0.001_10SFH_14day_75_160.csv\n",
      "-- Process for LS021_1.0_0.001_10SFH_5day_75_160.csv --\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n",
      "> Predicting on the validation set\n",
      "> Recording performance metrics\n",
      "Finished a fold.\n",
      "Starting a fold...\n",
      "> Assembling the datasets\n",
      "> Fitting the model on the training set\n"
     ]
    }
   ],
   "source": [
    "start = default_timer()\n",
    "\n",
    "model_names = ['LR036','LS021','SV009','SV088','SV130',\n",
    "               'SV085','SV059','LS001','LR043']  ## SPECIFY HERE\n",
    "training_sample_size = 1.0\n",
    "validation_sample_size = 0.001\n",
    "\n",
    "for tile_coiid in [('10SFH',75),('15TVG',1)]:\n",
    "    for scheme_name in ['14day','5day']:\n",
    "        for in_season in [160]:\n",
    "            for model_name in model_names:\n",
    "\n",
    "                model = return_model_object(model_name,\n",
    "                                bagged=True,\n",
    "                                n_estimators=100,\n",
    "                                sample_size=0.001,\n",
    "                                bootstrap=True)\n",
    " \n",
    "                p = {\n",
    "\n",
    "                    ## SPECIFY MODEL ##\n",
    "                    'model_name': model_name,\n",
    "                    'model': model,\n",
    "                    'training_sample_size': training_sample_size,\n",
    "                    'validation_sample_size': validation_sample_size,\n",
    "\n",
    "                    ## SPECIFY TILE AND SCHEME ##\n",
    "                    'tile': tile_coiid[0],\n",
    "                    'years': [2018, 2019, 2020, 2021, 2022],\n",
    "                    'scheme_name': scheme_name,\n",
    "                    'crop_of_interest_id': tile_coiid[1], \n",
    "                    'in_season': in_season\n",
    "                    }\n",
    "\n",
    "                #fit_predict_report(**p) # run with the above parameters\n",
    "                fit_predict_report(**p)\n",
    "                \n",
    "duration = default_timer() - start\n",
    "print(duration)\n",
    "with open(f'../data/times/time{start}.txt', 'a') as f:\n",
    "    f.write(str(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4a667-5c4a-43bd-8e37-688b783a9521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c5770-da77-4bc3-b583-0b9966f7082d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f405e9-434d-45ca-b265-f78d9844664e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2fed11-d7b2-4774-a39c-c8ee2394c8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b858981-bbfb-49f5-ba50-a122c51c045b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20657a-86c7-4b07-97ee-40b448120092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe98a48-30b4-450b-94b2-6c82dd458561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfced5b5-5f1e-4a3e-b4e6-6a978f251b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
